{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b67ebd66",
      "metadata": {
        "id": "b67ebd66"
      },
      "source": [
        "https://colab.research.google.com/github/Anutri03/eye/blob/main/new_blood_vessel.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "514e19a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "514e19a7",
        "outputId": "eb483c4c-1581-4979-e90f-4979e41a555d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'eye'...\n",
            "remote: Enumerating objects: 240, done.\u001b[K\n",
            "remote: Counting objects: 100% (240/240), done.\u001b[K\n",
            "remote: Compressing objects: 100% (231/231), done.\u001b[K\n",
            "remote: Total 240 (delta 19), reused 227 (delta 9), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (240/240), 32.86 MiB | 19.16 MiB/s, done.\n",
            "Resolving deltas: 100% (19/19), done.\n",
            "/content/eye/eye\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Anutri03/eye.git\n",
        "%cd eye\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "8b1235d8",
      "metadata": {
        "id": "8b1235d8"
      },
      "outputs": [],
      "source": [
        "import os##\n",
        "import cv2\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from operator import add\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2b22a5f",
      "metadata": {
        "id": "e2b22a5f"
      },
      "source": [
        "### Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "587cb251",
      "metadata": {
        "id": "587cb251"
      },
      "outputs": [],
      "source": [
        "class DriveDataset(Dataset):\n",
        "    def __init__(self, images_path, masks_path):\n",
        "\n",
        "        self.images_path = images_path\n",
        "        self.masks_path = masks_path\n",
        "        self.n_samples = len(images_path)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\" Reading image \"\"\"\n",
        "        image = cv2.imread(self.images_path[index], cv2.IMREAD_COLOR)\n",
        "        image = image/255.0\n",
        "        image = np.transpose(image, (2, 0, 1))\n",
        "        image = image.astype(np.float32)\n",
        "        image = torch.from_numpy(image)\n",
        "\n",
        "        \"\"\" Reading mask \"\"\"\n",
        "        mask = cv2.imread(self.masks_path[index], cv2.IMREAD_GRAYSCALE)\n",
        "        mask = mask/255.0\n",
        "        mask = np.expand_dims(mask, axis=0)\n",
        "        mask = mask.astype(np.float32)\n",
        "        mask = torch.from_numpy(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb5ca318",
      "metadata": {
        "id": "fb5ca318"
      },
      "source": [
        "### Unet helper classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "7f07053b",
      "metadata": {
        "id": "7f07053b"
      },
      "outputs": [],
      "source": [
        "# Build Conv_layer\n",
        "class conv_block(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_c)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_c)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "035531d3",
      "metadata": {
        "id": "035531d3"
      },
      "outputs": [],
      "source": [
        "# Build Encoder section\n",
        "class encoder_block(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = conv_block(in_c, out_c)\n",
        "        self.pool = nn.MaxPool2d((2, 2))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.conv(inputs)\n",
        "        p = self.pool(x)\n",
        "\n",
        "        return x, p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "ab892219",
      "metadata": {
        "id": "ab892219"
      },
      "outputs": [],
      "source": [
        "# Build Decoder section\n",
        "class decoder_block(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "\n",
        "        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
        "        self.conv = conv_block(out_c+out_c, out_c)\n",
        "\n",
        "    def forward(self, inputs, skip):\n",
        "        x = self.up(inputs)\n",
        "        x = torch.cat([x, skip], axis=1)\n",
        "        x = self.conv(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "23d54f4b",
      "metadata": {
        "id": "23d54f4b"
      },
      "outputs": [],
      "source": [
        "# Build Unet architecture\n",
        "class build_unet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.e1 = encoder_block(3, 64)\n",
        "        self.e2 = encoder_block(64, 128)\n",
        "        self.e3 = encoder_block(128, 256)\n",
        "        self.e4 = encoder_block(256, 512)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.b = conv_block(512, 1024)\n",
        "\n",
        "        # Decoder\n",
        "        self.d1 = decoder_block(1024, 512)\n",
        "        self.d2 = decoder_block(512, 256)\n",
        "        self.d3 = decoder_block(256, 128)\n",
        "        self.d4 = decoder_block(128, 64)\n",
        "\n",
        "        # Classifier\n",
        "        self.outputs = nn.Conv2d(64, 1, kernel_size=1, padding=0)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Encoder\n",
        "        s1, p1 = self.e1(inputs)\n",
        "        s2, p2 = self.e2(p1)\n",
        "        s3, p3 = self.e3(p2)\n",
        "        s4, p4 = self.e4(p3)\n",
        "\n",
        "        # Bottleneck\n",
        "        b = self.b(p4)\n",
        "\n",
        "        # Decoder\n",
        "        d1 = self.d1(b, s4)\n",
        "        d2 = self.d2(d1, s3)\n",
        "        d3 = self.d3(d2, s2)\n",
        "        d4 = self.d4(d3, s1)\n",
        "\n",
        "        outputs = self.outputs(d4)\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9e681ba",
      "metadata": {
        "id": "e9e681ba"
      },
      "source": [
        "### Losses classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "f84731aa",
      "metadata": {
        "id": "f84731aa"
      },
      "outputs": [],
      "source": [
        "# Dice Coefficient\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "\n",
        "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "        inputs = torch.sigmoid(inputs)\n",
        "\n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        intersection = (inputs * targets).sum()\n",
        "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
        "\n",
        "        return 1 - dice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "1762eb7a",
      "metadata": {
        "id": "1762eb7a"
      },
      "outputs": [],
      "source": [
        "# Dice Binary Cross Entropy Coefficient\n",
        "class DiceBCELoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(DiceBCELoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "\n",
        "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "        inputs = torch.sigmoid(inputs)\n",
        "\n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        intersection = (inputs * targets).sum()\n",
        "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
        "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
        "        Dice_BCE = BCE + dice_loss\n",
        "\n",
        "        return Dice_BCE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbeb7de2",
      "metadata": {
        "id": "cbeb7de2"
      },
      "source": [
        "### Function to seed the randomness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "2a86fd79",
      "metadata": {
        "id": "2a86fd79"
      },
      "outputs": [],
      "source": [
        "def seeding(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b2823e1",
      "metadata": {
        "id": "2b2823e1"
      },
      "source": [
        "### new dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "11b3243f",
      "metadata": {
        "id": "11b3243f"
      },
      "outputs": [],
      "source": [
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4915607",
      "metadata": {
        "id": "b4915607"
      },
      "source": [
        "### Function to calculate the taken time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "98bc8134",
      "metadata": {
        "id": "98bc8134"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04f46569",
      "metadata": {
        "id": "04f46569"
      },
      "source": [
        "### Function to train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "3f62e109",
      "metadata": {
        "id": "3f62e109"
      },
      "outputs": [],
      "source": [
        "def train(model, loader, optimizer, loss_fn, device):\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    model.train()\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, dtype=torch.float32)\n",
        "        y = y.to(device, dtype=torch.float32)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(x)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    epoch_loss = epoch_loss/len(loader)\n",
        "    return epoch_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edd7ffcf",
      "metadata": {
        "id": "edd7ffcf"
      },
      "source": [
        "### Function to evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "288742be",
      "metadata": {
        "id": "288742be"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, loader, loss_fn, device):\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device, dtype=torch.float32)\n",
        "            y = y.to(device, dtype=torch.float32)\n",
        "\n",
        "            y_pred = model(x)\n",
        "            loss = loss_fn(y_pred, y)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        epoch_loss = epoch_loss / len(loader)\n",
        "    return epoch_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e03dcf5a",
      "metadata": {
        "id": "e03dcf5a"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "0154f5f4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0154f5f4",
        "outputId": "df01d743-0483-4f25-9d2c-79bd36e34e31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Size:\n",
            "Train: 80 - Valid: 20\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Seeding\n",
        "seeding(42)\n",
        "\n",
        "# Create files directory to store checkpoint file\n",
        "create_dir(\"files\")\n",
        "\n",
        "#  Get data paths\n",
        "train_x = sorted(glob(\"/content/eye/Data/train/image/*\"))\n",
        "train_y = sorted(glob(\"/content/eye/Data/train/mask/*\"))\n",
        "\n",
        "valid_x = sorted(glob(\"/content/eye/Data/test/image/*\"))\n",
        "valid_y = sorted(glob(\"/content/eye/Data/test/mask/*\"))\n",
        "\n",
        "data_str = f\"Dataset Size:\\nTrain: {len(train_x)} - Valid: {len(valid_x)}\\n\"\n",
        "print(data_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set hyperparameters"
      ],
      "metadata": {
        "id": "3gk4Ttyazrpa"
      },
      "id": "3gk4Ttyazrpa"
    },
    {
      "cell_type": "code",
      "source": [
        "H = 512\n",
        "W = 512\n",
        "size = (H, W)\n",
        "batch_size = 2\n",
        "lr = 1e-4\n",
        "checkpoint_path = \"files/checkpoint.pth\""
      ],
      "metadata": {
        "id": "ngLPh-CJzuog"
      },
      "id": "ngLPh-CJzuog",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = DriveDataset(train_x, train_y)\n",
        "valid_dataset = DriveDataset(valid_x, valid_y)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "valid_loader = DataLoader(\n",
        "    dataset=valid_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")"
      ],
      "metadata": {
        "id": "Ee_uDcpxzxj_"
      },
      "id": "Ee_uDcpxzxj_",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unet Model"
      ],
      "metadata": {
        "id": "glUZaJAWz1I5"
      },
      "id": "glUZaJAWz1I5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set cuda device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Build model\n",
        "model = build_unet()\n",
        "model = model.to(device)\n",
        "\n",
        "# Set Optimizer and Loss\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)\n",
        "loss_fn = DiceBCELoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XggZucMQz4T1",
        "outputId": "38ea7ecb-d290-41cf-c3e3-05f5e502bf42"
      },
      "id": "XggZucMQz4T1",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model train\n",
        "\n",
        "\n",
        "best_valid_loss = float(\"inf\")\n",
        "num_epochs = 50\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_loader, optimizer, loss_fn, device)\n",
        "    valid_loss = evaluate(model, valid_loader, loss_fn, device)\n",
        "\n",
        "    # Saving the model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        print(f\"Valid loss improved from {best_valid_loss:2.4f} to {valid_loss:2.4f}. Saving checkpoint: {checkpoint_path}\")\n",
        "\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    data_str = f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\\n'\n",
        "    data_str += f'\\tTrain Loss: {train_loss:.3f}\\n'\n",
        "    data_str += f'\\tVal. Loss: {valid_loss:.3f}\\n'\n",
        "    print(data_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAuwFdZLz5iw",
        "outputId": "6d731dbd-6a0c-4ee8-8e05-6b77cc788ab9"
      },
      "id": "NAuwFdZLz5iw",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid loss improved from inf to 1.3119. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 01 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 1.180\n",
            "\tVal. Loss: 1.312\n",
            "\n",
            "Valid loss improved from 1.3119 to 0.9459. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 02 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.986\n",
            "\tVal. Loss: 0.946\n",
            "\n",
            "Valid loss improved from 0.9459 to 0.9094. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 03 | Epoch Time: 0m 36s\n",
            "\tTrain Loss: 0.923\n",
            "\tVal. Loss: 0.909\n",
            "\n",
            "Valid loss improved from 0.9094 to 0.8621. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 04 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.875\n",
            "\tVal. Loss: 0.862\n",
            "\n",
            "Valid loss improved from 0.8621 to 0.8084. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 05 | Epoch Time: 0m 36s\n",
            "\tTrain Loss: 0.834\n",
            "\tVal. Loss: 0.808\n",
            "\n",
            "Valid loss improved from 0.8084 to 0.7879. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 06 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.801\n",
            "\tVal. Loss: 0.788\n",
            "\n",
            "Valid loss improved from 0.7879 to 0.7461. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 07 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.763\n",
            "\tVal. Loss: 0.746\n",
            "\n",
            "Valid loss improved from 0.7461 to 0.7245. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 08 | Epoch Time: 0m 36s\n",
            "\tTrain Loss: 0.728\n",
            "\tVal. Loss: 0.725\n",
            "\n",
            "Valid loss improved from 0.7245 to 0.6890. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 09 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.696\n",
            "\tVal. Loss: 0.689\n",
            "\n",
            "Valid loss improved from 0.6890 to 0.6667. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 10 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.667\n",
            "\tVal. Loss: 0.667\n",
            "\n",
            "Valid loss improved from 0.6667 to 0.6258. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 11 | Epoch Time: 0m 36s\n",
            "\tTrain Loss: 0.640\n",
            "\tVal. Loss: 0.626\n",
            "\n",
            "Valid loss improved from 0.6258 to 0.6063. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 12 | Epoch Time: 0m 36s\n",
            "\tTrain Loss: 0.613\n",
            "\tVal. Loss: 0.606\n",
            "\n",
            "Valid loss improved from 0.6063 to 0.5868. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 13 | Epoch Time: 0m 36s\n",
            "\tTrain Loss: 0.588\n",
            "\tVal. Loss: 0.587\n",
            "\n",
            "Valid loss improved from 0.5868 to 0.5608. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 14 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.570\n",
            "\tVal. Loss: 0.561\n",
            "\n",
            "Valid loss improved from 0.5608 to 0.5424. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 15 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.545\n",
            "\tVal. Loss: 0.542\n",
            "\n",
            "Valid loss improved from 0.5424 to 0.5339. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 16 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.529\n",
            "\tVal. Loss: 0.534\n",
            "\n",
            "Valid loss improved from 0.5339 to 0.5125. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 17 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.511\n",
            "\tVal. Loss: 0.513\n",
            "\n",
            "Valid loss improved from 0.5125 to 0.4963. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 18 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.492\n",
            "\tVal. Loss: 0.496\n",
            "\n",
            "Valid loss improved from 0.4963 to 0.4808. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 19 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.478\n",
            "\tVal. Loss: 0.481\n",
            "\n",
            "Valid loss improved from 0.4808 to 0.4700. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 20 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.463\n",
            "\tVal. Loss: 0.470\n",
            "\n",
            "Valid loss improved from 0.4700 to 0.4654. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 21 | Epoch Time: 0m 36s\n",
            "\tTrain Loss: 0.449\n",
            "\tVal. Loss: 0.465\n",
            "\n",
            "Valid loss improved from 0.4654 to 0.4600. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 22 | Epoch Time: 0m 36s\n",
            "\tTrain Loss: 0.435\n",
            "\tVal. Loss: 0.460\n",
            "\n",
            "Valid loss improved from 0.4600 to 0.4404. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 23 | Epoch Time: 0m 36s\n",
            "\tTrain Loss: 0.425\n",
            "\tVal. Loss: 0.440\n",
            "\n",
            "Valid loss improved from 0.4404 to 0.4362. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 24 | Epoch Time: 0m 36s\n",
            "\tTrain Loss: 0.411\n",
            "\tVal. Loss: 0.436\n",
            "\n",
            "Valid loss improved from 0.4362 to 0.4301. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 25 | Epoch Time: 0m 36s\n",
            "\tTrain Loss: 0.401\n",
            "\tVal. Loss: 0.430\n",
            "\n",
            "Epoch: 26 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.393\n",
            "\tVal. Loss: 0.437\n",
            "\n",
            "Valid loss improved from 0.4301 to 0.4280. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 27 | Epoch Time: 0m 36s\n",
            "\tTrain Loss: 0.380\n",
            "\tVal. Loss: 0.428\n",
            "\n",
            "Valid loss improved from 0.4280 to 0.4185. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 28 | Epoch Time: 0m 36s\n",
            "\tTrain Loss: 0.368\n",
            "\tVal. Loss: 0.419\n",
            "\n",
            "Valid loss improved from 0.4185 to 0.4082. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 29 | Epoch Time: 0m 36s\n",
            "\tTrain Loss: 0.361\n",
            "\tVal. Loss: 0.408\n",
            "\n",
            "Epoch: 30 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.352\n",
            "\tVal. Loss: 0.409\n",
            "\n",
            "Valid loss improved from 0.4082 to 0.4045. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 31 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.339\n",
            "\tVal. Loss: 0.405\n",
            "\n",
            "Valid loss improved from 0.4045 to 0.4011. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 32 | Epoch Time: 0m 36s\n",
            "\tTrain Loss: 0.330\n",
            "\tVal. Loss: 0.401\n",
            "\n",
            "Epoch: 33 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.322\n",
            "\tVal. Loss: 0.404\n",
            "\n",
            "Epoch: 34 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.311\n",
            "\tVal. Loss: 0.416\n",
            "\n",
            "Valid loss improved from 0.4011 to 0.3928. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 35 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.305\n",
            "\tVal. Loss: 0.393\n",
            "\n",
            "Epoch: 36 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.296\n",
            "\tVal. Loss: 0.400\n",
            "\n",
            "Epoch: 37 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.289\n",
            "\tVal. Loss: 0.401\n",
            "\n",
            "Valid loss improved from 0.3928 to 0.3885. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 38 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.280\n",
            "\tVal. Loss: 0.389\n",
            "\n",
            "Epoch: 39 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.270\n",
            "\tVal. Loss: 0.391\n",
            "\n",
            "Epoch: 40 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.266\n",
            "\tVal. Loss: 0.390\n",
            "\n",
            "Epoch: 41 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.260\n",
            "\tVal. Loss: 0.391\n",
            "\n",
            "Valid loss improved from 0.3885 to 0.3857. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 42 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.255\n",
            "\tVal. Loss: 0.386\n",
            "\n",
            "Epoch: 43 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.249\n",
            "\tVal. Loss: 0.387\n",
            "\n",
            "Valid loss improved from 0.3857 to 0.3841. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 44 | Epoch Time: 0m 36s\n",
            "\tTrain Loss: 0.246\n",
            "\tVal. Loss: 0.384\n",
            "\n",
            "Epoch: 45 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.239\n",
            "\tVal. Loss: 0.390\n",
            "\n",
            "Epoch: 46 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.236\n",
            "\tVal. Loss: 0.385\n",
            "\n",
            "Epoch: 47 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.232\n",
            "\tVal. Loss: 0.386\n",
            "\n",
            "Valid loss improved from 0.3841 to 0.3814. Saving checkpoint: files/checkpoint.pth\n",
            "Epoch: 48 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.228\n",
            "\tVal. Loss: 0.381\n",
            "\n",
            "Epoch: 49 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.225\n",
            "\tVal. Loss: 0.387\n",
            "\n",
            "Epoch: 50 | Epoch Time: 0m 35s\n",
            "\tTrain Loss: 0.222\n",
            "\tVal. Loss: 0.383\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Testing"
      ],
      "metadata": {
        "id": "E9gC4etiE-6u"
      },
      "id": "E9gC4etiE-6u"
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(y_true, y_pred):\n",
        "    # Ground truth\n",
        "    y_true = y_true.cpu().numpy()\n",
        "    y_true = y_true > 0.5\n",
        "    y_true = y_true.astype(np.uint8)\n",
        "    y_true = y_true.reshape(-1)\n",
        "\n",
        "    # Prediction\n",
        "    y_pred = y_pred.cpu().numpy()\n",
        "    y_pred = y_pred > 0.5\n",
        "    y_pred = y_pred.astype(np.uint8)\n",
        "    y_pred = y_pred.reshape(-1)\n",
        "\n",
        "    score_jaccard = jaccard_score(y_true, y_pred)\n",
        "    score_f1 = f1_score(y_true, y_pred)\n",
        "    score_recall = recall_score(y_true, y_pred)\n",
        "    score_precision = precision_score(y_true, y_pred)\n",
        "    score_acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    return [score_jaccard, score_f1, score_recall, score_precision, score_acc]"
      ],
      "metadata": {
        "id": "5g9M91fdz9Ar"
      },
      "id": "5g9M91fdz9Ar",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g6B1hvhpFB3y"
      },
      "id": "g6B1hvhpFB3y",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}